{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cyberbullying_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac474d",
   "metadata": {},
   "source": [
    "## Data Exploration and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6836d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a random sample of the dataset\n",
    "df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values from both columns\n",
    "df = df[~df.tweet_text.isna()]\n",
    "df = df[~df.cyberbullying_type.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af27f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of labels\n",
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep tabs on how many rows we delete during cleaning\n",
    "initial_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3489cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df, column, n_bins):\n",
    "    fig, ax = plt.subplots()\n",
    "    hist, bin_edges = np.histogram(list(df[column].str.len()), bins=int(len(list(df[column].str.len()))/n_bins))\n",
    "    ax.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), edgecolor=\"black\", align=\"edge\")\n",
    "    plt.title(\"Histogram showing binned length of tweets\")\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Length of tweet')\n",
    "    plt.show()\n",
    "    return hist, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_values, bins = plot_histogram(df, 'tweet_text', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f17aa0",
   "metadata": {},
   "source": [
    "### The histogram shows an uneven distribution of data, showing outliers with large string length\n",
    "Let's clean up the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fed0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_values, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets with more than 350 characters\n",
    "df = df[df.tweet_text.str.len()< 300]\n",
    "\n",
    "# Remove tweets with less than 30 chars as the text might be too short to make a prediction on\n",
    "# df = df[~df.tweet_text.str.len()< 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297aea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_values, bins = plot_histogram(df, 'tweet_text', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fe259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove @tags as they are not important for prediction\n",
    "df.tweet_text = df.tweet_text.apply(lambda x: ' '.join(word for word in x.split(' ') if not word.startswith('@')))\n",
    "\n",
    "# Remove label which is not focused on any one type of context\n",
    "df = df[df.cyberbullying_type != 'other_cyberbullying']\n",
    "\n",
    "# Remove duplicated rows\n",
    "df = df[~df.tweet_text.duplicated()]\n",
    "\n",
    "# Operations like removing @tags can leave us with empty strings\n",
    "df = df[~(df.tweet_text == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09012abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have deleted {(initial_length - len(df))/initial_length*100}% of the initial dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80135b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a random sample of the dataset\n",
    "df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at some random tweets\n",
    "i=np.random.randint(len(df))\n",
    "print(i)\n",
    "df.iloc[i].tweet_text, df.iloc[i].cyberbullying_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cyberbullying_tweets_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47e538",
   "metadata": {},
   "source": [
    "### Example of issues Issues with the dataset (iloc, reason)\n",
    "21058: Not religion\n",
    "\n",
    "9002: News tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75912a4b",
   "metadata": {},
   "source": [
    "### TODO: Possible Feature Engineering\n",
    "- Check for caps letters (angry messages)\n",
    "- Isolate cuss words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "custom_stopwords = ['u', 'ur', 'i\\'m']\n",
    "for custom_word in custom_stopwords:\n",
    "    stopwords.words('english').append(custom_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet_text = df.tweet_text.apply(lambda x: ' '.join([word for word in x.lower().split(' ')\\\n",
    "                                                        if word not in stopwords.words('english')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "for label in df.cyberbullying_type.unique():\n",
    "    print(\"Word cloud for\", label)\n",
    "    # Get most commonly used words\n",
    "    common_words = Counter(\" \".join(df[df.cyberbullying_type == label].tweet_text).split()).most_common(10)\n",
    "    for item in common_words:\n",
    "        print(item)\n",
    "    \n",
    "    word_cloud = WordCloud(collocations = False, background_color = 'white')\\\n",
    "    .generate(' '.join(list(df[df.cyberbullying_type == label].tweet_text)))\n",
    "    # Display the generated Word Cloud\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
